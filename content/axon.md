+++
Categories = ["Computation", "Cognition", "Neuroscience", "Activation", "Learning", "Rubicon"]
bibfile = "ccnlab.json"
+++

**Axon** is an attempt to build a biologically accurate, yet still abstracted and computationally tractable model of neural computation in the mammalian brain (e.g., in rodents, monkeys, and people). The goal is to commit as few _errors of commission_ as possible: the features of the model should not violate any known, well-established properties of the brain. There is still plenty of room for _errors of omission_: not every detail can or should be included.

The approach is guided by an in-depth consideration of the available [[neuroscience]] literature, as well as a strong consideration for the functional and [[computation]]al [[levels of analysis]], informed by an understanding of the known [[cognition|cognitive]] and behavioral capabilities of the respective organisms. In other words, it is [[Computational Cognitive Neuroscience]], leveraging notable [[synergies]] across these different levels of analysis. These synergies provide some measure of confidence that the overall approach is on the right track toward answering the fundamental question: _how do the detailed biological mechanisms of the brain give rise to the computationally sophisticated cognitive abilities of the various mammalian species, including humans_?

Given the considerable scope of neuroscience, cognition, and computation covered here, all from coherent, self-consistent perspective, this material is suitable as an [[intro book|introductory textbook]]. This textbook treatment builds on a series of prior such [[book|textbooks]] going back to [[@^OReillyMunakata00]], which were based on the [[Leabra]] framework that Axon is the direct successor to.

Axon uses more realistic discrete spiking dynamics and associated learning mechanisms relative to Leabra, and supports more advanced brain area models to implement a functional version of the [[Rubicon]] goal-driven learning system, based on the interactions of an interconnected system of cortical and subcortical brain areas. This motivational system is critical for enabling autonomous learning of cognitive and behavioral skills in an ecologically-realistic manner.

A fundamental challenge for any theoretical understanding of something as complex as the brain, is that we suffer from significant _confirmation bias_ that causes us to contort the scientific data into a form that is consistent with our models, but is not actually how it works. Indeed, the [[bidirectional connectivity]] of the brain that is so central to our models contributes significantly to this phenomenon: it enables us to see what we want to see, not what is actually there. Indeed, we will levy this critique against several other alternative hypotheses along the way (e.g., see [[Hebbian learning]]).

In this context, a primary goal in generating the extensive content here on [compcogneuro.org](https://compcogneuro.org) is to document the full extent to which the current version of Axon captures the existing scientific findings in neuroscience and cognitive psychology, so that the interested reader may form their own opinion about the extent to which the model provides an accurate picture of brain function. Feedback on any and all such issues is encouraged, using the [github discussions](https://github.com/compcogneuro/web/discussions) forum. The [github issues](https://github.com/compcogneuro/web/issues) can be used to report typos or other such "bugs", and pull requests for suggested fixes or other contributions are always welcome, and provide a way to document contributions.

Ultimately, the definitive step in the scientific method is direct empirical tests of the specific predictions from the model, of which there have been a large number over the years, as documented in the relevant places herein. Perhaps the most central such test is reported in [[Jiang et al 2025]], which directly tests the [[temporal derivative]] form of [[synaptic plasticity]] that drives learning in the Axon model.

## Computational motivation

The computational-level process of [[search]] provides an overarching framework for motivating the neural mechanisms in Axon, from basic neural processing elements including spiking and synaptic weights, to the critical role of the goal-driven [[Rubicon]] system.

The central insight is that every cognitively-relevant computational problem or process can be formulated in terms of searching through a space of alternatives, to find a state that satisfies various criteria. For example, the game of chess can be understood in terms of searching through the very large space of possible board configurations for sequences of such configurations that lead to winning the game. Indeed, this is precisely what the [Deep Blue](https://en.wikipedia.org/wiki/Deep_Blue_(chess_computer)) chess computer did to beat grandmaster Gary Kasparov, via an optimized "brute force" search process.

The fundamental computational challenge for any search process is the [[curse of dimensionality]] as the size (dimensionality) of the space being searched increases. Each additional dimension adds an _exponential_ factor to the size of the space. For example, a binary space with 8 dimensions has $2^8 = 256$ states, but one with twice the number of dimensions (16) already has $2^{16} = 65,536$ elements. Any "real world" space almost certainly has an intractably large number of elements. See the [[search]] page for more details on how this relates to the computational complexity literature and the space of _NP-complete_ problems.

From this search-based perspective, the reason [[artificial intelligence]] (AI) used to be generally ineffective is because it relied on fundamentally _serial_, symbolic mechanisms to search through the relevant spaces, so it just broke down when confronting the complexity of the real world. And the reason AI is now successful is because [[abstract neural network|neural networks]] can perform efficient search in high-dimensional spaces, via _stochastic gradient descent_ operating on networks of _dedicated-parallel_ elements with weighted synaptic connections. There are two versions of this process, at different time-scales:

* [[Error backpropagation]] learning computes the _partial derivatives_ (i.e., gradients) across a network of interconnected neuron-like processing elements to efficiently search the space of possible representations (parameterized by weighted synaptic connections), to find those that enable specific problems to be solved. 

* [[Constraint satisfaction]] processing in [[bidirectional connectivity|bidirectionally connected]] networks also uses gradients to efficiently search the space of possible representations to find those that are consistent with both the constraints imposed by external inputs to the network, and constraints from the learned synaptic weights (i.e., the _knowledge_ in the network).

Thus, at this computational level, a major hypothesis behind the Axon framework is that these gradient-based search mechanisms using dedicated-parallel neural hardware are _essential_ for successful computation in the face of the complexity of the real world. Critically, most extant AI models, including the [[large language models]] powering ChatGPT and the like, depend on the power of error backpropagation learning, but do not have bidirectional connectivity and only have more limited forms of constraint satisfaction processing.

By contrast, the Axon framework is based foundationally on the bidirectional connectivity present in the [[neocortex]], which drives both the specific biologically-based form of error backpropagation it uses, and a full network-wide constraint satisfaction process at every step of neural processing. Conceptually, this means that an Axon network is computing with [[optimized representations]] at every step, which has important implications for how it behaves in challenging task environments. In effect, it can efficiently search a massive space of possible representations within roughly 200 ms, to find a way of solving problems that might otherwise take an intractable amount of time to find.

There are many reasons why existing abstract neural networks do not include bidirectional connectivity: it is computationally expensive, it requires robust [[inhibition|inhibitory]] and other mechanisms to control positive feedback loops, and, intriguingly, it requires discrete spiking neural dynamics to effectively represent graded and uncertain information, and allow for sampling of different modes of a distribution ([[@McKeeCrandellChaudhuriEtAl22]]). This is why the computational-level issue of efficient search seems to demand a rather biologically-detailed model.

The specific problem of serial trial-and-error behavior and learning, which is the domain of [[reinforcement learning]] (RL), poses a particularly critical challenge from the search perspective, because it is _inevitably_ serial. You can't take multiple different strategies to solve a problem at the same time, and you can't carry out multiple different action sequences at the same time. This _serial bottleneck_ of action ([[@Pashler94]]) makes it absolutely essential to employ highly optimized search processes wherever possible, to ensure that each such sequential step is as effective as possible. Otherwise, the curse of dimensionality will require an intractably large number of serial action steps to search spaces of even modest sizes.

Indeed, current RL algorithms require mind-boggling numbers of trials to learn even relatively simple video games, and much of the research is focused on ways of managing and distributing this massive search process. This has given RL a bad reputation among learning algorithms, but it is essential to appreciate that it is not that RL algorithms are inherently stupid: it is just that the RL domain is inherently intractable due to is serial-search nature.

In addition to the use of dedicated-parallel, gradient-based constraint-satisfaction processing at each time step, another critical way to mitigate the problem of serial action is to have an evolutionarily-optimized system of neural mechanisms that carefully guards the allocation of this precious serial search resource. This is the [[Rubicon]] goal-driven processing system in the Axon framework, which continuously learns about the benefit vs. cost tradeoffs associated with actions taken, and applies this knowledge to carefully evaluate the selection of new goals to pursue going forward.

You can blame this goal-driven part of your brain when you end up procrastinating instead of sinking a lot of time into a challenging or boring task. This is just your very rational, evolutionarily-optimized brain saying: "don't do something that takes a lot of time and effort, especially if it is not urgently necessary right now!" Instead, it makes much more sense to take on shorter, simpler tasks that you know you can do with high confidence, which will give you a decent little kick of [[dopamine]] when you knock them off the list.

Given that any organism living in the real world has to somehow solve the challenge of choosing sensible action plans based on an extremely limited ability to search the relevant problem space, understanding the full complement of neural mechanisms that make this work efficiently is thus a very high priority. This is reflected here in the significant effort invested to understand the often bewildering complexity of the extensive subcortical and cortical networks that implement the goal-driven action selection and engagement process.

## Axon mechanisms

The central elements of Axon in terms of neural and computational mechanisms are as follows (most of which are well established properties of the mammalian [[neocortex]]):

* [[neuron|Spiking neurons]] with relatively long-lasting [[neuron channels#NMDA]] and [[neuron channels#GABA-B]] channels that support [[stable activation]] states over the course of a roughly 200 msec [[theta rhythm|theta cycle]], which is essential for establishing a coherent representation of the current input state. This stability is necessary to drive effective learning using the temporally-extended [[temporal derivative]] mechanism.

    The Axon neuron model is fairly conventional from a computational neuroscience perspective, featuring two compartments (soma and [[neuron dendrites|dendrite]]) using a range of conductance-based electrophysiologically-accurate [[neuron channels|channels]] and the widely-used _AdEx_ adaptive exponential approximation to the full Hodgkin-Huxley spiking dynamics ([[@BretteGerstner05]]).

    The discrete spiking behavior of these neurons enables effective graded information integration over time in a way that continuous [[rate code activation]] communication does not, by allowing many different signals to be communicated over time, competing for the overall control of the network activation state as a function of the collective integration of spikes within the neurons in the network. As a result, Axon models are overall much more robust and well-behaved overall compared to their [[Leabra]] rate-code based counterparts, especially with respect to [[constraint satisfaction]] computation.

* [[Error-driven learning]] based on errors computed via a [[temporal derivative]] that naturally supports [[predictive learning]], as the difference over time of network activity states representing the prediction followed by the outcome. Local [[synaptic plasticity]] based on the competition between kinases updating at different rates, i.e., the [[kinase algorithm]], naturally computes the error gradient via the temporal derivative dynamic. The result is a fully biologically plausible form of the computationally powerful [[error backpropagation]] algorithm, as shown by the [[GeneRec]] algorithm. Initial empirical support for this mechanism is reported in [[Jiang et al 2025]], in electrophysiological measurements of synaptic plasticity in a rodent preparation.

    The combination of robust error-driven learning and biologically-detailed spiking neurons in Axon enables these neurons to learn to perform arbitrary computational and cognitive tasks. Furthermore, the availability of a clear computational measure of performance in terms of overall learning capability across a wide range of tasks has enabled the optimization of all the biological parameters to maximize learning performance. There is a consistent set of such parameters that generally works best across all the tasks investigated to date, and thus the additional degrees of freedom associated with these parameters are generally eliminated from consideration in constructing new models, greatly reducing the effective degrees of freedom of the model.

* [[Bidirectional connectivity]] among excitatory neurons, which is necessary for propagating error signals throughout the network, and pooled [[inhibition]] which is necessary for controlling the effects of bidirectional excitatory connectivity, while also having beneficial computational effects in terms of [[attention]] and competition. As noted above, bidirectional connectivity supports [[constraint satisfaction]] dynamics that can efficiently [[search]] through large high-dimensional knowledge spaces to find (and synthesize) the most relevant information given the current bottom-up (sensory) and top-down (goals) constraints. This results in the use of [[optimized representations]] at each step of processing, which has significant computational advantages.

    Perhaps most importantly, this bidirectional connectivity is widely thought to be essential for [[conscious awareness]] ([[@Lamme06]]), which is likely critical for the system to access its own internal state of knowledge. This ability is notably absent in current [[abstract neural network]] models that drive the widely-used [[large language models]] (LLMs) for example, which are notorious for their inability to accurately evaluate their own knowledge states, resulting in significant _confabulation_. Most experts do not think these models are conscious, which is consistent with the fact that they are based exclusively on feedforward connectivity.

    The central role of bidirectional connectivity in Axon represents one of the most important points of divergence relative to the vast majority of existing neural network models (along with the combination of biologically-detailed spiking dynamics and error-driven learning), and testing the functional importance of this property is a major overarching goal of this research.

These neural mechanisms provide the dedicated-parallel, gradient-based foundation for the learning and processing associated with the posterior [[neocortex]], which learns powerful representations for encoding, predicting, and inferring states of the world around us. Specific models illustrating these abilities are discussed in the context of spatially invariant [[object recognition]], [[spatial attention]], and [[language]]. See the discussion on [[combinatorial vs conjunctive]] representations for a summary of the computational-level properties of these learned representations, and how they support effective [[generalization]] to novel situations.

## General intelligence and motivation

The parallel, distributed processing of the posterior neocortex is only part of the overall picture for understanding the full scope of mammalian intelligent behavior. As noted above, there are substantial evolutionarily optimized subcortical and cortical mechanisms that carefully guard the serial aspects of cognition and behavior, which emerge out of the parallel brain mechanisms over time. These serial modes, while inefficient relative to dedicated-parallel search, have the unique advantage of being significantly more _flexible_.

Indeed the seemingly-magical universal computational capacity of the [[Turing machine]] derives precisely from the intrinsic flexibility of serial processing, where arbitrary sequences of basic information processing operations can be combined to accomplish any task. Thus, in order to understand the full power of human intelligence (e.g., to create a true [[artificial intelligence#artificial general intelligence]] or  AGI), we also need to understand how this serial processing emerges from the parallel neural substrates.

The current focus in the Axon project is on understanding the goal and motivation-related brain areas in rodents, which have clear homologs to corresponding areas in the primate brain, but everything is much smaller and simpler. Although nobody has accused rodents of having fully general intelligence, they nevertheless have significant survival abilities that depend on a kind of "street smarts", and we believe this is a critical element of human general intelligence as well. Considering what it actually takes for people to solve challenging problems, it seems clear that AGI requires more than just raw [[generalization]]: it also depends critically on motivational systems to drive cognition toward solutions, and to focus efforts on the most important problems (i.e., "grit"; [[@Dweck08]]).

These goal and motivation-related brain areas include the [[prefrontal cortex]], [[basal ganglia]], and [[amygdala]], which support a powerful and flexible capacity for goal-driven planning and problem solving in rodents (and occupy about a third of the total brain mass, indicating the importance of these areas). Consistent with the need to carefully guard the expensive and valuable serial processing resource, this goal-driven system is sensitive to the overall costs and benefits of actions, in order to commit time and effort to a consistent plan of behavior to accomplish a given goal. 

The Axon model of this system is called [[Rubicon]], based on the evidence that goal engagement produces a major change in the operative _value function_ governing behavior, which serves to maintain goal focus and drives strongly valenced disappointment and even depression from failure to accomplish goals ([[@HeckhausenGollwitzer87]]; [[@OReillyHazyMollickEtAl14]]; [[@OReilly20]]). This framework integrates a wide range of neuroscience, cognitive, and computational data to provide a robust model of how a simulated rodent learns from the outcomes of temporally extended sequences of actions driven by a goal-engaged state. This goal-driven learning provides important error signals that shape learning throughout the network, going beyond what is possible with basic predictive error-driven learning.

Without these mechanisms, the neocortex alone would be a largely passive learning and perceiving system without the ability to organize behavior effectively over time (basically what you experience while dreaming, when the [[prefrontal cortex]] is selectively deactivated).

At a computational level, the domain of [[reinforcement learning#model-based]] reinforcement learning has significantly overlapping objectives, but most of the work in this area is insufficiently robust to the  [[curse of dimensionality]] associated with trying to [[search]] through high-dimensional spaces for appropriate goals and action plans. For example, many model-based models employ sequential predictive "roll-outs" using internal models (e.g., [[@HafnerLillicrapNorouziEtAl22]]) to anticipate the possible effects of a sequence of actions (much like the _Deep Blue_ chess playing computer), but this is  intractable in real-world environments.

By contrast, the Rubicon framework is based on using a gradient-based, dedicated-parallel constraint-satisfaction search at each timestep, leveraging the bidirectional connectivity of the neocortical network, including its prefrontal cortex components. The resulting [[optimized representations]] automatically encode the environment in a way that is strongly constrained by the current goals and plans, allowing novel affordances and problem solutions to emerge.

This dynamic is essentially what we experience subjectively in our [[conscious awareness]], and our analysis suggests that this is not merely an epiphenomenon, but rather an essential reflection of a system that has pervasive access to the contents of its own state of knowledge at every moment, and can use all of these constraints from all over the brain to shape the flow of "thought" to solve problems in ways that purely feedforward architectures cannot.

## From rodent to human

After the rodent level model is fully developed, the next steps involve scaling up to the human level, progressively via intermediate primate-level models, such as that of the widely-studied macaque monkey. Macaques have highly-developed visual perceptual pathways and saccadic motor control networks that provide an active, dynamic modality for constructing increasingly rich internal models of the world. The [[deepvision simulation]] of sensory predictive learning provides an initial attempt to model this system.

In addition to the ability to form and mentally manipulate much richer and more dynamic mental models of the world, the human brain can leverage the incredible power of [[language]], which is widely recognized as an essential ingredient for advanced thought and reasoning abilities. The success of LLMs in developing remarkably capable internal representations of the world through predictive learning of language clearly demonstrates this point, and the Axon / Rubicon framework would add a strong goal-driven motivational system and all of the above-noted abilities to consciously access its own state of knowledge, potentially resolving the longstanding issues with confabulation.

Computationally, the combination of language and a strong goal-driven planning system enables the human brain to function like a [[self-programmable]] [[Turing machine]] that can organize its own behavior in highly flexible, systematic ways. In short, we suggest that the key to full human-level AGI is having this serial, highly flexible, universal form of computation riding on top of a fully bidirectionally connected parallel processing foundation that automatically generates optimized representations through constraint satisfaction, all driven by a strong goal-driven motivational system that keeps everything on track and focused on the most relevant issues.

<!--- TODO: turing with a twist citations! -->

There will be many practical challenges associated with scaling up these models, given their additional biological realism and complexity relative to existing more abstract models. Furthermore, the capacity to actually accomplish significant levels of general problem solving ability in humans requires years of learning and education, representing the apex of the developmental stages mapped out by [[@^Piaget41]]. Thus, this remains a major challenge, but the current objective is to establish a solid biological, computational, and cognitive foundation to understand the essential ingredients of what makes humans so smart.

## Summary

We can summarize the overall approach by way of answering several key questions that one might reasonably ask in evaluating a theoretical and computational model of the human brain:

1. _Is it scientifically accurate?_ The Axon mechanisms are all grounded in detailed [[neuroscience]] data, and produce known [[cognition|cognitive]] and behavioral phenomena accurately. There are no significant errors of commission in the mechanisms included: each such mechanism has solid evidence in support of it, including critically the basis for powerful error-driven learning via a [[temporal derivative]] computed by the competition between a faster LTP-promoting CaMKII pathway and a slower opposing LTD-promoting DAPK1 pathway in the [[kinase algorithm]]. An initial experimental test of this hypothesis ([[Jiang et al 2025]]) shows consistent evidence.

2. _Does it have a clear principled basis for effective computation?_ The principle of [[search]] through high-dimensional spaces unifies our understanding of both learning and online computation through [[optimized representations]] (via [[constraint satisfaction]] supported by [[bidirectional connectivity]]). The principal challenge is the [[curse of dimensionality]], which requires _dedicated-parallel_, _gradient-based_ search mechanisms. Most of the existing [[reinforcement learning#model-based]] reinforcement learning mechanisms do not scale well because they involve serial search of one form or another. By contrast, the [[Rubicon]] framework leverages the parallel mechanisms in Axon, along with a neuroscience-based [[computational-cognitive-neuroscience#reverse engineering]] of the results of millions of years of parallel [[evolution|evolutionary]] search to build in stronger [[bias-variance tradeoff|biases]] that shape and constrain goal-driven learning.

    From a cognitive perspective, the most direct, intuitive basis for optimism about the overall approach is the connection to [[conscious awareness]] and its grounding in widespread bidirectional interactions among brain areas ([[@Lamme06]]), which suggests that this system could actually _think_ and _reason_ in the same kind of conscious, deliberative manner that we intuitively understand to be taking place in the human brain. Critically, this system should be able to directly access its own state of knowledge in a way that existing models cannot, and use this to direct further reasoning and problem-solving steps via goal-driven cognitive control mechanisms.

3. _Does it actually work in practice, at least at smaller scale?_ Extensive small-scale [[simulations]] demonstrate that the implemented models do work in practice, including demonstrating significant benefits from the spiking-based activations in Axon relative to the earlier [[rate-code activation]] in Leabra and most [[abstract neural network]] models. 

4. _Does it scale effectively?_ The considerable additional computation involved in simulating the constraint satisfaction process via bidirectional excitatory connectivity on each trial of processing, along with the biologically-detailed neural activity functions, represents a significant challenge for scaling up this approach. Thus, the necessary strategy is to establish as strong of a foundation in answering the first 3 questions, to the point that it is then possible to invest the resources necessary to take on this final, definitive step. Existing efforts have at least put the entire computation on parallel GPU hardware in a highly efficient manner, so some of the necessary infrastructure is in place, but much more work remains.

